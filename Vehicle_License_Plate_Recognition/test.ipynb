{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import keras\n",
    "from PIL import Image\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Input\n",
    "from keras.layers import AveragePooling2D, Flatten, GlobalMaxPooling2D\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "config={'黑': 36, 'B': 19, '桂': 45, '2': 62, '陕': 29, '浙': 48, 'N': 3, '1': 58, 'K': 20, 'T': 6, '津': 49, '闽': 44, 'X': 17, '粤': 47, 'Q': 16, 'V': 15, '琼': 41, '皖': 46, '沪': 32, '冀': 52, '鲁': 50, '贵': 35, '川': 31, '吉': 25, '豫': 34, '6': 57, 'L': 21, '5': 63, '晋': 28, '4': 60, 'E': 18, '云': 38, 'S': 7, 'J': 12, 'G': 4, '赣': 30, 'A': 8, 'D': 14, '湘': 40, '鄂': 51, '0': 55, '蒙': 43, 'Y': 22, '辽': 37, 'U': 2, '3': 61, '9': 54, 'W': 24, 'Z': 5, 'P': 23, 'F': 9, 'M': 11, '8': 59, '7': 56, 'R': 1, 'H': 10, '青': 27, 'C': 13, '苏': 33, '甘': 42, '宁': 26, '京': 53, '渝': 39}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(path=\"车牌字符识别训练数据\"):\n",
    "    fi = []\n",
    "    rt = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f != '车牌字符识别训练数据' and f != '.DS_Store':\n",
    "                # 路徑：os.path.join(root, f)\n",
    "                im = cv2.resize(cv2.imread(os.path.join(root, f)), (24, 48), interpolation=cv2.INTER_CUBIC)\n",
    "                fi.append(im)\n",
    "                rt.append(root.split('/')[-1])\n",
    "                # print(cv2.imread(os.path.join(root, f)))\n",
    "                # new = fname[0] + 'b' + fname[1]\n",
    "                # os.rename(os.path.join(rt, f), os.path.join(rt, new))\n",
    "    print(len(fi))\n",
    "    return fi, rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(file_name, data_labels, data_datas):\n",
    "    writer = tf.python_io.TFRecordWriter(file_name + \".tfrecords\")\n",
    "    num = len(data_labels)\n",
    "    for i in range(num):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"以處理{0}數據集{1}張\".format(file_name, i + 1))\n",
    "        label = data_labels[i].encode()\n",
    "        data = np.resize(data_datas[i], [1, 3456])[0].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            \"label\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[label])),\n",
    "            'img_val': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data]))\n",
    "        }))  # example對象 對label及img_val 進行封裝\n",
    "        writer.write(example.SerializeToString())\n",
    "        i += 1\n",
    "    print(\"{0}數據集處理完成\".format(file_name))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tfrecord():\n",
    "    # 將製作好的 tfrecord 數據集文件讀取出來,並轉換成圖片,以驗證數據是否準確無誤\n",
    "    from PIL import Image\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([\"train.tfrecords\"])  # 讀入數據流\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)  # 返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.string),\n",
    "                                           'img_val': tf.FixedLenFeature([], tf.string),\n",
    "                                       })  # 取出包含image和label的feature对象\n",
    "    image = tf.decode_raw(features['img_val'], tf.uint8)\n",
    "    label = tf.cast(features['label'], tf.string)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    % matplotlib inline\n",
    "\n",
    "    with tf.Session() as sess:  # 開始一個對話\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(5):\n",
    "            example, l = sess.run([image, label])  # 在會話中取出image和label數據\n",
    "            img=np.resize(example, [48, 24, 3])\n",
    "            plt.title(\"The picture number is \" + str(l.decode('utf-8')))\n",
    "#             plt.figure(figsize=(2,4))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "# print_tfrecord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_two_color(name):\n",
    "    img = cv2.imread(name)\n",
    "    grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    a = 0\n",
    "    for i in grayImage:\n",
    "        b = 0\n",
    "        for j in i:\n",
    "            if j < grayImage.mean():  # 比对均值\n",
    "                grayImage[a][b] = 0\n",
    "            else:\n",
    "                grayImage[a][b] = 255\n",
    "            b += 1\n",
    "        a += 1\n",
    "        del b\n",
    "    cv2.imwrite(name, grayImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    from random import shuffle\n",
    "    filedata, filelabel = get_data_list('车牌字符识别训练数据')\n",
    "    x = [i for i in range(len(filelabel))] # 18499\n",
    "    shuffle(x)\n",
    "    a=[];b=[]\n",
    "    for i in x[:13000]:\n",
    "        a.append(filelabel[i])\n",
    "        b.append(filedata[i])\n",
    "    write_tfrecords('train',a,b)\n",
    "    a=[];b=[]\n",
    "    for i in x[13000:16400]:\n",
    "        a.append(filelabel[i])\n",
    "        b.append(filedata[i])\n",
    "    write_tfrecords('validation',a,b)\n",
    "    a=[];b=[]\n",
    "    for i in x[16400:]:\n",
    "        a.append(filelabel[i])\n",
    "        b.append(filedata[i])\n",
    "    write_tfrecords('test',a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfrecord(filename='train.tfrecords'):\n",
    "    # 將製作好的 tfrecord 數據集文件讀取出來,並轉換成圖片,以驗證數據是否準確無誤\n",
    "    from PIL import Image\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([filename])  # 讀入數據流\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)  # 返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.string),\n",
    "                                           'img_val': tf.FixedLenFeature([], tf.string),\n",
    "                                       })  # 取出包含image和label的feature对象\n",
    "    image = tf.decode_raw(features['img_val'], tf.uint8)\n",
    "    label = tf.cast(features['label'], tf.string)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    imgdata=[];imglabel=[]\n",
    "    with tf.Session() as sess:  # 開始一個對話\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(1000):\n",
    "            example, l = sess.run([image, label])  # 在會話中取出image和label數據\n",
    "            data = np.resize(example, [48, 24, 3])\n",
    "            data=turn_two_color(data)\n",
    "            tem = np.zeros((1, 64))\n",
    "            tem[0][config[l.decode('utf-8')]] = 1\n",
    "            imgdata.append(data)\n",
    "            imglabel.append(tem[0])\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    return imgdata,imglabel\n",
    "\n",
    "\n",
    "def turn_two_color(data):\n",
    "    import cv2\n",
    "    grayImage = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    a = 0\n",
    "    for i in grayImage:\n",
    "        b = 0\n",
    "        for j in i:\n",
    "            if j < grayImage.mean():  # 比对均值\n",
    "                grayImage[a][b] = 0\n",
    "            else:\n",
    "                grayImage[a][b] = 255\n",
    "            b += 1\n",
    "        a += 1\n",
    "        del b\n",
    "    return grayImage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D 塊，雙路雙卷積計算\n",
    "    :param input_tensor:(tensor) 輸入張量\n",
    "    :param filters:(tuple) 卷積核打包\n",
    "    :param strides:(int) 卷積步長\n",
    "    :param BN_axis:(int) 規範化卷積軸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # 解包卷積核數量\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    y = Conv2D(filters3, (1, 1), strides=strides, name=Conv_base_name + '1a')(input_tensor)\n",
    "    y = BatchNormalization(axis=bn_axis, name=BN_base_name + '1b')(y)\n",
    "\n",
    "    x = layers.add([x, y])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D 塊，雙路單卷積計算\n",
    "    :param input_tensor:(tensor) 輸入張量\n",
    "    :param filters:(tuple) 卷積核打包\n",
    "    :param strides:(int) 卷積步長\n",
    "    :param BN_axis:(int) 規範化卷積軸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # 解包卷積核數量\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def my_resnet():\n",
    "    inputs = Input(shape=(1, 24, 48))\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=4, kernel_size=(2, 4), padding='same', name='Conv1', data_format='channels_first')(inputs)\n",
    "    x = BatchNormalization(axis=1, name='BN_Conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format='channels_first')(x)\n",
    "\n",
    "    x = conv_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='a')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='b')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='c')\n",
    "\n",
    "    # x = conv_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='a')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='b')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='c')\n",
    "\n",
    "    x = AveragePooling2D((2, 2), name='avg_pool')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='softmax', name='softmax')(x)\n",
    "#     x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    model = Model(inputs, x, name='My_Resnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"返回一個已創建好的 resnet model\"\"\"\n",
    "    model = my_resnet()\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train=55000, test=5000, batch_size=128, epochs=2):\n",
    "    trimages, trlabels=get_tfrecord()\n",
    "    model = create_model()\n",
    "    # from keras.utils import plot_model\n",
    "    # plot_model(model, to_file='./Resnet_model.png')\n",
    "    tem = model.fit(trimages, trlabels, batch_size=batch_size, epochs=epochs, )\n",
    "#     print(tem.history)\n",
    "\n",
    "#     model.evaluate(tsimages, tslabels, batch_size=batch_size)\n",
    "    model.save('./resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1000 arrays: [array([[  0, 255, 255, ...,   0,   0,   0],\n       [255, 255, 255, ...,   0,   0,   0],\n       [255, 255, 255, ..., 255,   0,   0],\n       ..., \n       [255, 255, 255, ..., 255, 255, 255],\n       [  ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-b4aa4b4356ca>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train, test, batch_size, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# from keras.utils import plot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# plot_model(model, to_file='./Resnet_model.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(tem.history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tudoudou/Documents/python/BIg_work/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tudoudou/Documents/python/BIg_work/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1408\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tudoudou/Documents/python/BIg_work/env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                  \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                  \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1000 arrays: [array([[  0, 255, 255, ...,   0,   0,   0],\n       [255, 255, 255, ...,   0,   0,   0],\n       [255, 255, 255, ..., 255,   0,   0],\n       ..., \n       [255, 255, 255, ..., 255, 255, 255],\n       [  ..."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
