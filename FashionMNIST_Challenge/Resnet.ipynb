{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FashionMNIST Challenge 之 深度殘差神經網絡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、算法原理\n",
    "        咳咳,先不說算法原理,先來介紹一下這個算法深度殘差網絡是什麼.深度殘差網絡是由4個華人在2015年提出的深度卷積網絡,一經出世,便在ImageNet大賽中一舉拿下圖像分類、檢測、定位的三項世界冠軍.\n",
    "        接下來開始以我微薄的認知來說一說這個模型,如有錯誤,歡迎批評指正.\n",
    "        我們都知道增加網絡的寬度和深度都可以很好的提高網絡的性能,一般來說,深的網絡比淺的網絡效果要好,或者說深的網絡效果至少可以達到淺的網絡效果,原因很簡單,我們可以嘗試把淺的模型完全遷移到深的模型前n層,而深的模型的後幾層則全部由等價影射組成,這樣就可以達到和淺的網絡一樣的效果.那假設後面的那幾層不是等價影射,而是繼續計算呢?這個例子就好比VGG,該網絡就是在AlexNet的基礎上通過增加網絡深度而大大的提高了整個網絡的性能.\n",
    "        我们都知道增加网络的宽度和深度可以很好的提高网络的性能，深的网络一般都比浅的的网络效果好，比如说一个深的网络A和一个浅的网络B，那A的性能至少都能跟B一样，为什么呢？因为就算我们把A的网络参数全部迁移到B的前面几层，而B后面的层只是做一个等价的映射，就达到了A网络的一样的效果。一个比较好的例子就是VGG，该网络就是在AlexNex的基础上通过增加网络深度大幅度提高了网络性能。 \n",
    "        但是事實卻並非如此,就想一個限度,超過了限度,反而結果會變得更加糟糕,這種情況被稱為degradation.\n",
    "<img src=\"http://img.blog.csdn.net/20161028170240163\"  alt=\"http://blog.csdn.net/scety/article/details/52957991\">\n",
    "    \n",
    "        這是由於性能退化造成的,於是殘差網絡便橫空出世,很好的解決了這個問題.ResNet採用了跳躍結構來作為網路的基本結構。\n",
    "<img src='img/11073image.png'>\n",
    "\n",
    "        使用ResNet對比.\n",
    "\n",
    "<img src=\"img/20161203160636938.png\">\n",
    "\n",
    "        再來一個對比.\n",
    "<img src='img/20161203160822610.png'>\n",
    "\n",
    "        至於為什麼要使用這種結構,使用這個結構有明顯的改善效果,我認為，原本我們要優化的目標是H(x)=F(x)+x（x就是該結構的輸入）,但是通過這種結構以後就把優化的目標由H(x)轉化為H(x)-x。我之前提到，深網路在淺網路的基礎上只要上面幾層做一個等價映射就可以達到淺網路同樣的效果，但是終究不行的原因，或許是因為我們的算法很難將其訓練到那個程度，也就是沒辦法將上面幾層訓練到一個等價映射， 以至於深網路最後達到了一個更差的效果。 那麼這時，我們把訓練目標轉變，由原來的H(x)轉為H(x)-x，因為這時候就不是把上面幾層訓練到一個等價映射了，而是將其逼近與0，這樣訓練的難度比訓練到一個等價映射應該下降了很多。也就是說，在一個網路中（假設有5層），如果前面四層已經達到一個最優的函數，那第五層就是沒有必要的了，這時我們通過這種跳躍結構，我們的優化目標就從一個等價映射變為逼近0了，逼近其他任何函數都會造成網路退化。 通過這種方式就可以解決網路太深難訓練的問題。(這段稍為抄了一下).\n",
    "\n",
    ">Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x),we let the stacked nonlinear layers fit another mapping of F(x): H(x)-x. The original mapping is recast into F(x)+x.\n",
    "\n",
    "        至於下面算法不使用ResNet18和ResNet50,而選擇了自己造一個殘差網絡,原因是原本的resent輸入層過大,且為3通道,在此使用計算量過大而且我的計算機根本跑不動啊!!!😭😭所以早了一個輕量級的殘差網絡,效果最高可以達到90.5% .\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、算法實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Input\n",
    "from keras.layers import AveragePooling2D, Flatten, GlobalMaxPooling2D\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def conv_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D 塊，雙路雙卷積計算\n",
    "    :param input_tensor:(tensor) 輸入張量\n",
    "    :param filters:(tuple) 卷積核打包\n",
    "    :param strides:(int) 卷積步長\n",
    "    :param BN_axis:(int) 規範化卷積軸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # 解包卷積核數量\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    y = Conv2D(filters3, (1, 1), strides=strides, name=Conv_base_name + '1a')(input_tensor)\n",
    "    y = BatchNormalization(axis=bn_axis, name=BN_base_name + '1b')(y)\n",
    "\n",
    "    x = layers.add([x, y])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D 塊，雙路單卷積計算\n",
    "    :param input_tensor:(tensor) 輸入張量\n",
    "    :param filters:(tuple) 卷積核打包\n",
    "    :param strides:(int) 卷積步長\n",
    "    :param BN_axis:(int) 規範化卷積軸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # 解包卷積核數量\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def my_resnet():\n",
    "    inputs = Input(shape=(1, 28, 28))\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=4, kernel_size=(3, 3), padding='same', name='Conv1', data_format='channels_first')(inputs)\n",
    "    x = BatchNormalization(axis=1, name='BN_Conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format='channels_first')(x)\n",
    "\n",
    "    x = conv_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='a')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='b')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='c')\n",
    "    \n",
    "    # x = conv_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='a')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='b')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='c')\n",
    "\n",
    "    x = AveragePooling2D((2, 2), name='avg_pool')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10, activation='softmax', name='softmax10')(x)\n",
    "    # x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    model = Model(inputs, x, name='My_Resnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"返回一個已創建好的 resnet model\"\"\"\n",
    "    # model = keras.applications.resnet50.ResNet50(include_top=True, weights=None,\n",
    "    #                                              input_tensor=None, input_shape=(224, 224, 3),\n",
    "    #                                              pooling='max',\n",
    "    #                                              classes=10)\n",
    "    model = my_resnet()\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(filename, tensor=[1, 784], num=5000):\n",
    "    \"\"\"\n",
    "    讀取tfrecord文件數據\n",
    "    :param filename: 文件名\n",
    "    :param tensor: 緯度\n",
    "    :param num: 讀取數據數量\n",
    "    :return: images 圖片數據列表    labels(numpy) 標籤列表    maxlabels(int) 標籤所在項\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_val': tf.FixedLenFeature(tensor, tf.float32),\n",
    "                                       })\n",
    "    image = tf.cast(features['img_val'], tf.float64)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    images = []\n",
    "    labels = []\n",
    "    maxlabels = []\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(num):\n",
    "            example, l = sess.run([image, label])\n",
    "            arr = np.array(example[0])\n",
    "            arr = arr.reshape((28, 28))\n",
    "            images.append([arr])\n",
    "            tem = np.zeros((1, 10))\n",
    "            tem[0][l] = 1.0\n",
    "            labels.append(tem[0])\n",
    "            maxlabels.append([l])\n",
    "            del tem\n",
    "        coord.request_stop()\n",
    "        coord.join(threads=threads)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, maxlabels\n",
    "\n",
    "\n",
    "def trun(arr):\n",
    "    out_arr = []\n",
    "    for img in arr:\n",
    "        img = Image.fromarray(img.reshape([28, 28])).resize([224, 224])\n",
    "        img = np.asarray(img)\n",
    "        im = []\n",
    "        for a in img:\n",
    "            row = []\n",
    "            for b in a:\n",
    "                row.append([b, b + 0.0, b + 0.0])\n",
    "            im.append(row)\n",
    "            del row\n",
    "        out_arr.append(im)\n",
    "        del im\n",
    "    return out_arr\n",
    "\n",
    "\n",
    "def getdata(train=55000, test=5000):\n",
    "    trimages, trlabels, _ = read_tfrecord('train.tfrecords', num=train, )\n",
    "    teimages, telabels, _ = read_tfrecord('test.tfrecords', num=test, )\n",
    "    # trimages = trun(trimages)\n",
    "    # teimages = trun(teimages)\n",
    "\n",
    "    return trimages, trlabels, teimages, telabels\n",
    "\n",
    "\n",
    "def main(train=55000, test=5000, batch_size=128, epochs=20):\n",
    "    trimages, trlabels, tsimages, tslabels = getdata(train=train, test=test)\n",
    "    model = create_model()\n",
    "    # from keras.utils import plot_model\n",
    "    # plot_model(model, to_file='./Resnet_model.png')\n",
    "    tem = model.fit(trimages, trlabels, batch_size=batch_size, epochs=epochs, )\n",
    "    paint(tem.history)\n",
    "\n",
    "    model.evaluate(tsimages, tslabels, batch_size=batch_size)\n",
    "    model.save('./resnet.h5')\n",
    "\n",
    "\n",
    "def paint(hist):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    # fig = plt.figure(figsize=(6, 3))\n",
    "    ti = []\n",
    "    for i in range(len(hist['loss'])):\n",
    "        ti.append(i)\n",
    "    plt.plot(ti, hist['loss'], c='red')\n",
    "    plt.plot(ti, hist['acc'], c='blue')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_model(num=1000.0):\n",
    "    timages, tlabels, _ = read_tfrecord('validation.tfrecords', num=int(num))\n",
    "\n",
    "    model = load_model('./resnet_.h5')\n",
    "    pre = model.predict(timages)\n",
    "    tr = 0\n",
    "    for a, b in zip(tlabels, pre):\n",
    "        a = np.argmax(a)\n",
    "        b = np.argmax(b)\n",
    "        if a == b:\n",
    "            tr += 1\n",
    "    ls = tr / num\n",
    "    print(\"正確率為：{0}\".format(ls))\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    timages, tlabels, _ = read_tfrecord('validation.tfrecords', num=int(10000))\n",
    "    model = load_model('./resnet_.h5')\n",
    "    model.fit(timages,tlabels,batch_size=100,epochs=10)\n",
    "    model.evaluate(timages, tlabels, batch_size=100)\n",
    "    model.save('./resnet__.h5')\n",
    "\n",
    "\n",
    "def test():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    data = input_data.read_data_sets('./data/', validation_size=5000)\n",
    "    data = data.train.next_batch(5000)\n",
    "    img = []\n",
    "    label = []\n",
    "    for i in data[0]:\n",
    "        i = np.reshape(i, [28, 28])\n",
    "        img.append(i)\n",
    "    print(\"轉換完成\")\n",
    "    for i in data[1]:\n",
    "        tem = np.zeros((1, 10))\n",
    "        tem[0][i] = 1.0\n",
    "        label.append(tem[0])\n",
    "    print('轉換完成')\n",
    "    model = create_model()\n",
    "    model.fit(img, label, batch_size=50, epochs=5, )\n",
    "    model.evaluate(img, label, batch_size=50)\n",
    "    model.save('./resnet_test.h5')\n",
    "\n",
    "def write():\n",
    "    f=open('./key.txt','w')\n",
    "    timages, tlabels, _ = read_tfrecord('test.tfrecords', num=int(100))\n",
    "\n",
    "    model = load_model('./resnet.h5')\n",
    "    pre = model.predict(timages)\n",
    "    for i in pre:\n",
    "        i=str(np.argmax(i))\n",
    "        f.writelines(i+'\\r')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.7545 - acc: 0.7467\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 365s 7ms/step - loss: 0.4089 - acc: 0.8563\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.3542 - acc: 0.8741\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.3236 - acc: 0.8839\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.3028 - acc: 0.8913\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 657s 12ms/step - loss: 0.2883 - acc: 0.8964\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 380s 7ms/step - loss: 0.2763 - acc: 0.9017\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 380s 7ms/step - loss: 0.2632 - acc: 0.9046\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 375s 7ms/step - loss: 0.2553 - acc: 0.9081\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 383s 7ms/step - loss: 0.2471 - acc: 0.9103\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 392s 7ms/step - loss: 0.2394 - acc: 0.9136\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 367s 7ms/step - loss: 0.2330 - acc: 0.9154\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 369s 7ms/step - loss: 0.2270 - acc: 0.9179\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.2220 - acc: 0.9195\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.2165 - acc: 0.9208\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.2125 - acc: 0.9230\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 384s 7ms/step - loss: 0.2074 - acc: 0.9245\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 387s 7ms/step - loss: 0.2031 - acc: 0.9260\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 391s 7ms/step - loss: 0.1990 - acc: 0.9278\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.1963 - acc: 0.9280\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1919 - acc: 0.9294\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 353s 6ms/step - loss: 0.1898 - acc: 0.9309\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1864 - acc: 0.9323\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.1831 - acc: 0.9340\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 375s 7ms/step - loss: 0.1806 - acc: 0.9342\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 374s 7ms/step - loss: 0.1783 - acc: 0.9343\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.1755 - acc: 0.9349\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 1444s 26ms/step - loss: 0.1725 - acc: 0.9378\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1699 - acc: 0.9387\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1682 - acc: 0.9384\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 370s 7ms/step - loss: 0.1656 - acc: 0.9405\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 364s 7ms/step - loss: 0.1642 - acc: 0.9395\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 366s 7ms/step - loss: 0.1611 - acc: 0.9411\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 358s 7ms/step - loss: 0.1602 - acc: 0.9411\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 362s 7ms/step - loss: 0.1574 - acc: 0.9422\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 360s 7ms/step - loss: 0.1559 - acc: 0.9431\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.1546 - acc: 0.9436\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 352s 6ms/step - loss: 0.1526 - acc: 0.9437\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 367s 7ms/step - loss: 0.1501 - acc: 0.9454\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.1498 - acc: 0.9454\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.1476 - acc: 0.9463\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 513s 9ms/step - loss: 0.1468 - acc: 0.9462\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.1448 - acc: 0.9476\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 726s 13ms/step - loss: 0.1430 - acc: 0.9475\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.1411 - acc: 0.9481\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 361s 7ms/step - loss: 0.1392 - acc: 0.9485\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1389 - acc: 0.9489\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 352s 6ms/step - loss: 0.1378 - acc: 0.9495\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 362s 7ms/step - loss: 0.1355 - acc: 0.9502\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1356 - acc: 0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"ckend\" on line 1 in\n",
      "/Users/tudoudou/.matplotlib/matplotlibrc.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(epochs=50)\n",
    "\n",
    "    # test()\n",
    "    # test_model(num=5000)\n",
    "    # x_train = np.random.random((2,5,5))\n",
    "    # print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正確率為：0.901\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、實踐總結\n",
    "1. 先上個上面算法的模型圖陣陣場\n",
    "<img src=\"./img/Resnet_model.png\" height=\"2000px\" width=\"300px\">\n",
    "2. 結果就是上不去,一定不是模型的問題(畫圈圈),一定是數據集在搞鬼,一定是的\n",
    "\n",
    "參考連結:\n",
    "1. [深度残差网络（ResNet）浅析](http://blog.csdn.net/scety/article/details/52957991)\n",
    "2. [為什麼ResNet和DenseNet可以這麼深](http://bangqu.com/QuxRV5.html)\n",
    "3. [无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构](https://www.jiqizhixin.com/articles/2017-08-19-4)\n",
    "\n",
    "#### 完成人: 曹國鴻 完成時間:2017年11月25日"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
