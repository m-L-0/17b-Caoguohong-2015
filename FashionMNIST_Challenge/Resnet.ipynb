{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FashionMNIST Challenge ä¹‹ æ·±åº¦æ®˜å·®ç¥ç¶“ç¶²çµ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€ç®—æ³•åŸç†\n",
    "        å’³å’³,å…ˆä¸èªªç®—æ³•åŸç†,å…ˆä¾†ä»‹ç´¹ä¸€ä¸‹é€™å€‹ç®—æ³•æ·±åº¦æ®˜å·®ç¶²çµ¡æ˜¯ä»€éº¼.æ·±åº¦æ®˜å·®ç¶²çµ¡æ˜¯ç”±4å€‹è¯äººåœ¨2015å¹´æå‡ºçš„æ·±åº¦å·ç©ç¶²çµ¡,ä¸€ç¶“å‡ºä¸–,ä¾¿åœ¨ImageNetå¤§è³½ä¸­ä¸€èˆ‰æ‹¿ä¸‹åœ–åƒåˆ†é¡ã€æª¢æ¸¬ã€å®šä½çš„ä¸‰é …ä¸–ç•Œå† è».\n",
    "        æ¥ä¸‹ä¾†é–‹å§‹ä»¥æˆ‘å¾®è–„çš„èªçŸ¥ä¾†èªªä¸€èªªé€™å€‹æ¨¡å‹,å¦‚æœ‰éŒ¯èª¤,æ­¡è¿æ‰¹è©•æŒ‡æ­£.\n",
    "        æˆ‘å€‘éƒ½çŸ¥é“å¢åŠ ç¶²çµ¡çš„å¯¬åº¦å’Œæ·±åº¦éƒ½å¯ä»¥å¾ˆå¥½çš„æé«˜ç¶²çµ¡çš„æ€§èƒ½,ä¸€èˆ¬ä¾†èªª,æ·±çš„ç¶²çµ¡æ¯”æ·ºçš„ç¶²çµ¡æ•ˆæœè¦å¥½,æˆ–è€…èªªæ·±çš„ç¶²çµ¡æ•ˆæœè‡³å°‘å¯ä»¥é”åˆ°æ·ºçš„ç¶²çµ¡æ•ˆæœ,åŸå› å¾ˆç°¡å–®,æˆ‘å€‘å¯ä»¥å˜—è©¦æŠŠæ·ºçš„æ¨¡å‹å®Œå…¨é·ç§»åˆ°æ·±çš„æ¨¡å‹å‰nå±¤,è€Œæ·±çš„æ¨¡å‹çš„å¾Œå¹¾å±¤å‰‡å…¨éƒ¨ç”±ç­‰åƒ¹å½±å°„çµ„æˆ,é€™æ¨£å°±å¯ä»¥é”åˆ°å’Œæ·ºçš„ç¶²çµ¡ä¸€æ¨£çš„æ•ˆæœ.é‚£å‡è¨­å¾Œé¢çš„é‚£å¹¾å±¤ä¸æ˜¯ç­‰åƒ¹å½±å°„,è€Œæ˜¯ç¹¼çºŒè¨ˆç®—å‘¢?é€™å€‹ä¾‹å­å°±å¥½æ¯”VGG,è©²ç¶²çµ¡å°±æ˜¯åœ¨AlexNetçš„åŸºç¤ä¸Šé€šéå¢åŠ ç¶²çµ¡æ·±åº¦è€Œå¤§å¤§çš„æé«˜äº†æ•´å€‹ç¶²çµ¡çš„æ€§èƒ½.\n",
    "        æˆ‘ä»¬éƒ½çŸ¥é“å¢åŠ ç½‘ç»œçš„å®½åº¦å’Œæ·±åº¦å¯ä»¥å¾ˆå¥½çš„æé«˜ç½‘ç»œçš„æ€§èƒ½ï¼Œæ·±çš„ç½‘ç»œä¸€èˆ¬éƒ½æ¯”æµ…çš„çš„ç½‘ç»œæ•ˆæœå¥½ï¼Œæ¯”å¦‚è¯´ä¸€ä¸ªæ·±çš„ç½‘ç»œAå’Œä¸€ä¸ªæµ…çš„ç½‘ç»œBï¼Œé‚£Açš„æ€§èƒ½è‡³å°‘éƒ½èƒ½è·ŸBä¸€æ ·ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºå°±ç®—æˆ‘ä»¬æŠŠAçš„ç½‘ç»œå‚æ•°å…¨éƒ¨è¿ç§»åˆ°Bçš„å‰é¢å‡ å±‚ï¼Œè€ŒBåé¢çš„å±‚åªæ˜¯åšä¸€ä¸ªç­‰ä»·çš„æ˜ å°„ï¼Œå°±è¾¾åˆ°äº†Aç½‘ç»œçš„ä¸€æ ·çš„æ•ˆæœã€‚ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ä¾‹å­å°±æ˜¯VGGï¼Œè¯¥ç½‘ç»œå°±æ˜¯åœ¨AlexNexçš„åŸºç¡€ä¸Šé€šè¿‡å¢åŠ ç½‘ç»œæ·±åº¦å¤§å¹…åº¦æé«˜äº†ç½‘ç»œæ€§èƒ½ã€‚ \n",
    "        ä½†æ˜¯äº‹å¯¦å»ä¸¦éå¦‚æ­¤,å°±æƒ³ä¸€å€‹é™åº¦,è¶…éäº†é™åº¦,åè€Œçµæœæœƒè®Šå¾—æ›´åŠ ç³Ÿç³•,é€™ç¨®æƒ…æ³è¢«ç¨±ç‚ºdegradation.\n",
    "<img src=\"http://img.blog.csdn.net/20161028170240163\"  alt=\"http://blog.csdn.net/scety/article/details/52957991\">\n",
    "    \n",
    "        é€™æ˜¯ç”±æ–¼æ€§èƒ½é€€åŒ–é€ æˆçš„,æ–¼æ˜¯æ®˜å·®ç¶²çµ¡ä¾¿æ©«ç©ºå‡ºä¸–,å¾ˆå¥½çš„è§£æ±ºäº†é€™å€‹å•é¡Œ.ResNetæ¡ç”¨äº†è·³èºçµæ§‹ä¾†ä½œç‚ºç¶²è·¯çš„åŸºæœ¬çµæ§‹ã€‚\n",
    "<img src='img/11073image.png'>\n",
    "\n",
    "        ä½¿ç”¨ResNetå°æ¯”.\n",
    "\n",
    "<img src=\"img/20161203160636938.png\">\n",
    "\n",
    "        å†ä¾†ä¸€å€‹å°æ¯”.\n",
    "<img src='img/20161203160822610.png'>\n",
    "\n",
    "        è‡³æ–¼ç‚ºä»€éº¼è¦ä½¿ç”¨é€™ç¨®çµæ§‹,ä½¿ç”¨é€™å€‹çµæ§‹æœ‰æ˜é¡¯çš„æ”¹å–„æ•ˆæœ,æˆ‘èªç‚ºï¼ŒåŸæœ¬æˆ‘å€‘è¦å„ªåŒ–çš„ç›®æ¨™æ˜¯H(x)=F(x)+xï¼ˆxå°±æ˜¯è©²çµæ§‹çš„è¼¸å…¥ï¼‰,ä½†æ˜¯é€šéé€™ç¨®çµæ§‹ä»¥å¾Œå°±æŠŠå„ªåŒ–çš„ç›®æ¨™ç”±H(x)è½‰åŒ–ç‚ºH(x)-xã€‚æˆ‘ä¹‹å‰æåˆ°ï¼Œæ·±ç¶²è·¯åœ¨æ·ºç¶²è·¯çš„åŸºç¤ä¸Šåªè¦ä¸Šé¢å¹¾å±¤åšä¸€å€‹ç­‰åƒ¹æ˜ å°„å°±å¯ä»¥é”åˆ°æ·ºç¶²è·¯åŒæ¨£çš„æ•ˆæœï¼Œä½†æ˜¯çµ‚ç©¶ä¸è¡Œçš„åŸå› ï¼Œæˆ–è¨±æ˜¯å› ç‚ºæˆ‘å€‘çš„ç®—æ³•å¾ˆé›£å°‡å…¶è¨“ç·´åˆ°é‚£å€‹ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯æ²’è¾¦æ³•å°‡ä¸Šé¢å¹¾å±¤è¨“ç·´åˆ°ä¸€å€‹ç­‰åƒ¹æ˜ å°„ï¼Œ ä»¥è‡³æ–¼æ·±ç¶²è·¯æœ€å¾Œé”åˆ°äº†ä¸€å€‹æ›´å·®çš„æ•ˆæœã€‚ é‚£éº¼é€™æ™‚ï¼Œæˆ‘å€‘æŠŠè¨“ç·´ç›®æ¨™è½‰è®Šï¼Œç”±åŸä¾†çš„H(x)è½‰ç‚ºH(x)-xï¼Œå› ç‚ºé€™æ™‚å€™å°±ä¸æ˜¯æŠŠä¸Šé¢å¹¾å±¤è¨“ç·´åˆ°ä¸€å€‹ç­‰åƒ¹æ˜ å°„äº†ï¼Œè€Œæ˜¯å°‡å…¶é€¼è¿‘èˆ‡0ï¼Œé€™æ¨£è¨“ç·´çš„é›£åº¦æ¯”è¨“ç·´åˆ°ä¸€å€‹ç­‰åƒ¹æ˜ å°„æ‡‰è©²ä¸‹é™äº†å¾ˆå¤šã€‚ä¹Ÿå°±æ˜¯èªªï¼Œåœ¨ä¸€å€‹ç¶²è·¯ä¸­ï¼ˆå‡è¨­æœ‰5å±¤ï¼‰ï¼Œå¦‚æœå‰é¢å››å±¤å·²ç¶“é”åˆ°ä¸€å€‹æœ€å„ªçš„å‡½æ•¸ï¼Œé‚£ç¬¬äº”å±¤å°±æ˜¯æ²’æœ‰å¿…è¦çš„äº†ï¼Œé€™æ™‚æˆ‘å€‘é€šéé€™ç¨®è·³èºçµæ§‹ï¼Œæˆ‘å€‘çš„å„ªåŒ–ç›®æ¨™å°±å¾ä¸€å€‹ç­‰åƒ¹æ˜ å°„è®Šç‚ºé€¼è¿‘0äº†ï¼Œé€¼è¿‘å…¶ä»–ä»»ä½•å‡½æ•¸éƒ½æœƒé€ æˆç¶²è·¯é€€åŒ–ã€‚ é€šéé€™ç¨®æ–¹å¼å°±å¯ä»¥è§£æ±ºç¶²è·¯å¤ªæ·±é›£è¨“ç·´çš„å•é¡Œã€‚(é€™æ®µç¨ç‚ºæŠ„äº†ä¸€ä¸‹).\n",
    "\n",
    ">Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x),we let the stacked nonlinear layers fit another mapping of F(x): H(x)-x. The original mapping is recast into F(x)+x.\n",
    "\n",
    "        è‡³æ–¼ä¸‹é¢ç®—æ³•ä¸ä½¿ç”¨ResNet18å’ŒResNet50,è€Œé¸æ“‡äº†è‡ªå·±é€ ä¸€å€‹æ®˜å·®ç¶²çµ¡,åŸå› æ˜¯åŸæœ¬çš„resentè¼¸å…¥å±¤éå¤§,ä¸”ç‚º3é€šé“,åœ¨æ­¤ä½¿ç”¨è¨ˆç®—é‡éå¤§è€Œä¸”æˆ‘çš„è¨ˆç®—æ©Ÿæ ¹æœ¬è·‘ä¸å‹•å•Š!!!ğŸ˜­ğŸ˜­æ‰€ä»¥æ—©äº†ä¸€å€‹è¼•é‡ç´šçš„æ®˜å·®ç¶²çµ¡,æ•ˆæœæœ€é«˜å¯ä»¥é”åˆ°90.5% .\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€ç®—æ³•å¯¦ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Input\n",
    "from keras.layers import AveragePooling2D, Flatten, GlobalMaxPooling2D\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def conv_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D å¡Šï¼Œé›™è·¯é›™å·ç©è¨ˆç®—\n",
    "    :param input_tensor:(tensor) è¼¸å…¥å¼µé‡\n",
    "    :param filters:(tuple) å·ç©æ ¸æ‰“åŒ…\n",
    "    :param strides:(int) å·ç©æ­¥é•·\n",
    "    :param BN_axis:(int) è¦ç¯„åŒ–å·ç©è»¸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # è§£åŒ…å·ç©æ ¸æ•¸é‡\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    y = Conv2D(filters3, (1, 1), strides=strides, name=Conv_base_name + '1a')(input_tensor)\n",
    "    y = BatchNormalization(axis=bn_axis, name=BN_base_name + '1b')(y)\n",
    "\n",
    "    x = layers.add([x, y])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, bn_axis, filters, phase, name, strides=(1, 1)):\n",
    "    \"\"\"\n",
    "    Conv2D å¡Šï¼Œé›™è·¯å–®å·ç©è¨ˆç®—\n",
    "    :param input_tensor:(tensor) è¼¸å…¥å¼µé‡\n",
    "    :param filters:(tuple) å·ç©æ ¸æ‰“åŒ…\n",
    "    :param strides:(int) å·ç©æ­¥é•·\n",
    "    :param BN_axis:(int) è¦ç¯„åŒ–å·ç©è»¸\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters  # è§£åŒ…å·ç©æ ¸æ•¸é‡\n",
    "    Conv_base_name = 'Conv_' + name + '_' + str(phase) + '_phase_'\n",
    "    BN_base_name = 'BN_' + name + '_' + str(phase) + '_phase_'\n",
    "    x = Conv2D(\n",
    "        filters=filters1, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2a'\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2a')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters2, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2b'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2b')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=filters3, kernel_size=(1, 1), strides=strides, name=Conv_base_name + '2c'\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=BN_base_name + '2c')(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    a = Activation('relu')(x)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def my_resnet():\n",
    "    inputs = Input(shape=(1, 28, 28))\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=4, kernel_size=(3, 3), padding='same', name='Conv1', data_format='channels_first')(inputs)\n",
    "    x = BatchNormalization(axis=1, name='BN_Conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format='channels_first')(x)\n",
    "\n",
    "    x = conv_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='a')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='b')\n",
    "    x = identity_block(input_tensor=x, bn_axis=1, filters=(4, 4, 64), phase=2, name='c')\n",
    "    \n",
    "    # x = conv_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='a')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='b')\n",
    "    # x = identity_block(input_tensor=x, bn_axis=1, filters=(8, 8, 512), phase=3, name='c')\n",
    "\n",
    "    x = AveragePooling2D((2, 2), name='avg_pool')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10, activation='softmax', name='softmax10')(x)\n",
    "    # x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    model = Model(inputs, x, name='My_Resnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"è¿”å›ä¸€å€‹å·²å‰µå»ºå¥½çš„ resnet model\"\"\"\n",
    "    # model = keras.applications.resnet50.ResNet50(include_top=True, weights=None,\n",
    "    #                                              input_tensor=None, input_shape=(224, 224, 3),\n",
    "    #                                              pooling='max',\n",
    "    #                                              classes=10)\n",
    "    model = my_resnet()\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(filename, tensor=[1, 784], num=5000):\n",
    "    \"\"\"\n",
    "    è®€å–tfrecordæ–‡ä»¶æ•¸æ“š\n",
    "    :param filename: æ–‡ä»¶å\n",
    "    :param tensor: ç·¯åº¦\n",
    "    :param num: è®€å–æ•¸æ“šæ•¸é‡\n",
    "    :return: images åœ–ç‰‡æ•¸æ“šåˆ—è¡¨    labels(numpy) æ¨™ç±¤åˆ—è¡¨    maxlabels(int) æ¨™ç±¤æ‰€åœ¨é …\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_val': tf.FixedLenFeature(tensor, tf.float32),\n",
    "                                       })\n",
    "    image = tf.cast(features['img_val'], tf.float64)\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    images = []\n",
    "    labels = []\n",
    "    maxlabels = []\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for i in range(num):\n",
    "            example, l = sess.run([image, label])\n",
    "            arr = np.array(example[0])\n",
    "            arr = arr.reshape((28, 28))\n",
    "            images.append([arr])\n",
    "            tem = np.zeros((1, 10))\n",
    "            tem[0][l] = 1.0\n",
    "            labels.append(tem[0])\n",
    "            maxlabels.append([l])\n",
    "            del tem\n",
    "        coord.request_stop()\n",
    "        coord.join(threads=threads)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels, maxlabels\n",
    "\n",
    "\n",
    "def trun(arr):\n",
    "    out_arr = []\n",
    "    for img in arr:\n",
    "        img = Image.fromarray(img.reshape([28, 28])).resize([224, 224])\n",
    "        img = np.asarray(img)\n",
    "        im = []\n",
    "        for a in img:\n",
    "            row = []\n",
    "            for b in a:\n",
    "                row.append([b, b + 0.0, b + 0.0])\n",
    "            im.append(row)\n",
    "            del row\n",
    "        out_arr.append(im)\n",
    "        del im\n",
    "    return out_arr\n",
    "\n",
    "\n",
    "def getdata(train=55000, test=5000):\n",
    "    trimages, trlabels, _ = read_tfrecord('train.tfrecords', num=train, )\n",
    "    teimages, telabels, _ = read_tfrecord('test.tfrecords', num=test, )\n",
    "    # trimages = trun(trimages)\n",
    "    # teimages = trun(teimages)\n",
    "\n",
    "    return trimages, trlabels, teimages, telabels\n",
    "\n",
    "\n",
    "def main(train=55000, test=5000, batch_size=128, epochs=20):\n",
    "    trimages, trlabels, tsimages, tslabels = getdata(train=train, test=test)\n",
    "    model = create_model()\n",
    "    # from keras.utils import plot_model\n",
    "    # plot_model(model, to_file='./Resnet_model.png')\n",
    "    tem = model.fit(trimages, trlabels, batch_size=batch_size, epochs=epochs, )\n",
    "    paint(tem.history)\n",
    "\n",
    "    model.evaluate(tsimages, tslabels, batch_size=batch_size)\n",
    "    model.save('./resnet.h5')\n",
    "\n",
    "\n",
    "def paint(hist):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    # fig = plt.figure(figsize=(6, 3))\n",
    "    ti = []\n",
    "    for i in range(len(hist['loss'])):\n",
    "        ti.append(i)\n",
    "    plt.plot(ti, hist['loss'], c='red')\n",
    "    plt.plot(ti, hist['acc'], c='blue')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_model(num=1000.0):\n",
    "    timages, tlabels, _ = read_tfrecord('validation.tfrecords', num=int(num))\n",
    "\n",
    "    model = load_model('./resnet_.h5')\n",
    "    pre = model.predict(timages)\n",
    "    tr = 0\n",
    "    for a, b in zip(tlabels, pre):\n",
    "        a = np.argmax(a)\n",
    "        b = np.argmax(b)\n",
    "        if a == b:\n",
    "            tr += 1\n",
    "    ls = tr / num\n",
    "    print(\"æ­£ç¢ºç‡ç‚ºï¼š{0}\".format(ls))\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    timages, tlabels, _ = read_tfrecord('validation.tfrecords', num=int(10000))\n",
    "    model = load_model('./resnet_.h5')\n",
    "    model.fit(timages,tlabels,batch_size=100,epochs=10)\n",
    "    model.evaluate(timages, tlabels, batch_size=100)\n",
    "    model.save('./resnet__.h5')\n",
    "\n",
    "\n",
    "def test():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    data = input_data.read_data_sets('./data/', validation_size=5000)\n",
    "    data = data.train.next_batch(5000)\n",
    "    img = []\n",
    "    label = []\n",
    "    for i in data[0]:\n",
    "        i = np.reshape(i, [28, 28])\n",
    "        img.append(i)\n",
    "    print(\"è½‰æ›å®Œæˆ\")\n",
    "    for i in data[1]:\n",
    "        tem = np.zeros((1, 10))\n",
    "        tem[0][i] = 1.0\n",
    "        label.append(tem[0])\n",
    "    print('è½‰æ›å®Œæˆ')\n",
    "    model = create_model()\n",
    "    model.fit(img, label, batch_size=50, epochs=5, )\n",
    "    model.evaluate(img, label, batch_size=50)\n",
    "    model.save('./resnet_test.h5')\n",
    "\n",
    "def write():\n",
    "    f=open('./key.txt','w')\n",
    "    timages, tlabels, _ = read_tfrecord('test.tfrecords', num=int(100))\n",
    "\n",
    "    model = load_model('./resnet.h5')\n",
    "    pre = model.predict(timages)\n",
    "    for i in pre:\n",
    "        i=str(np.argmax(i))\n",
    "        f.writelines(i+'\\r')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.7545 - acc: 0.7467\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 365s 7ms/step - loss: 0.4089 - acc: 0.8563\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.3542 - acc: 0.8741\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.3236 - acc: 0.8839\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.3028 - acc: 0.8913\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 657s 12ms/step - loss: 0.2883 - acc: 0.8964\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 380s 7ms/step - loss: 0.2763 - acc: 0.9017\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 380s 7ms/step - loss: 0.2632 - acc: 0.9046\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 375s 7ms/step - loss: 0.2553 - acc: 0.9081\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 383s 7ms/step - loss: 0.2471 - acc: 0.9103\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 392s 7ms/step - loss: 0.2394 - acc: 0.9136\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 367s 7ms/step - loss: 0.2330 - acc: 0.9154\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 369s 7ms/step - loss: 0.2270 - acc: 0.9179\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.2220 - acc: 0.9195\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.2165 - acc: 0.9208\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.2125 - acc: 0.9230\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 384s 7ms/step - loss: 0.2074 - acc: 0.9245\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 387s 7ms/step - loss: 0.2031 - acc: 0.9260\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 391s 7ms/step - loss: 0.1990 - acc: 0.9278\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.1963 - acc: 0.9280\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1919 - acc: 0.9294\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 353s 6ms/step - loss: 0.1898 - acc: 0.9309\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1864 - acc: 0.9323\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.1831 - acc: 0.9340\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 375s 7ms/step - loss: 0.1806 - acc: 0.9342\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 374s 7ms/step - loss: 0.1783 - acc: 0.9343\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.1755 - acc: 0.9349\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 1444s 26ms/step - loss: 0.1725 - acc: 0.9378\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1699 - acc: 0.9387\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1682 - acc: 0.9384\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 370s 7ms/step - loss: 0.1656 - acc: 0.9405\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 364s 7ms/step - loss: 0.1642 - acc: 0.9395\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 366s 7ms/step - loss: 0.1611 - acc: 0.9411\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 358s 7ms/step - loss: 0.1602 - acc: 0.9411\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 362s 7ms/step - loss: 0.1574 - acc: 0.9422\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 360s 7ms/step - loss: 0.1559 - acc: 0.9431\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 359s 7ms/step - loss: 0.1546 - acc: 0.9436\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 352s 6ms/step - loss: 0.1526 - acc: 0.9437\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 367s 7ms/step - loss: 0.1501 - acc: 0.9454\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 354s 6ms/step - loss: 0.1498 - acc: 0.9454\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.1476 - acc: 0.9463\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 513s 9ms/step - loss: 0.1468 - acc: 0.9462\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 351s 6ms/step - loss: 0.1448 - acc: 0.9476\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 726s 13ms/step - loss: 0.1430 - acc: 0.9475\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 356s 6ms/step - loss: 0.1411 - acc: 0.9481\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 361s 7ms/step - loss: 0.1392 - acc: 0.9485\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 355s 6ms/step - loss: 0.1389 - acc: 0.9489\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 352s 6ms/step - loss: 0.1378 - acc: 0.9495\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 362s 7ms/step - loss: 0.1355 - acc: 0.9502\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 363s 7ms/step - loss: 0.1356 - acc: 0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"ckend\" on line 1 in\n",
      "/Users/tudoudou/.matplotlib/matplotlibrc.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(epochs=50)\n",
    "\n",
    "    # test()\n",
    "    # test_model(num=5000)\n",
    "    # x_train = np.random.random((2,5,5))\n",
    "    # print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£ç¢ºç‡ç‚ºï¼š0.901\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€å¯¦è¸ç¸½çµ\n",
    "1. å…ˆä¸Šå€‹ä¸Šé¢ç®—æ³•çš„æ¨¡å‹åœ–é™£é™£å ´\n",
    "<img src=\"./img/Resnet_model.png\" height=\"2000px\" width=\"300px\">\n",
    "2. çµæœå°±æ˜¯ä¸Šä¸å»,ä¸€å®šä¸æ˜¯æ¨¡å‹çš„å•é¡Œ(ç•«åœˆåœˆ),ä¸€å®šæ˜¯æ•¸æ“šé›†åœ¨æé¬¼,ä¸€å®šæ˜¯çš„\n",
    "\n",
    "åƒè€ƒé€£çµ:\n",
    "1. [æ·±åº¦æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰æµ…æ](http://blog.csdn.net/scety/article/details/52957991)\n",
    "2. [ç‚ºä»€éº¼ResNetå’ŒDenseNetå¯ä»¥é€™éº¼æ·±](http://bangqu.com/QuxRV5.html)\n",
    "3. [æ— éœ€æ•°å­¦èƒŒæ™¯ï¼Œè¯»æ‡‚ ResNetã€Inception å’Œ Xception ä¸‰å¤§å˜é©æ€§æ¶æ„](https://www.jiqizhixin.com/articles/2017-08-19-4)\n",
    "\n",
    "#### å®Œæˆäºº: æ›¹åœ‹é´» å®Œæˆæ™‚é–“:2017å¹´11æœˆ25æ—¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
